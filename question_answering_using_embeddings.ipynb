{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal language model POCs and OpenAI API demo\n",
    "\n",
    "Prepared for Ventera brownbag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tiktoken\n",
    "\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "openai.api_key = \"sk-yourkey\" # TODO: Convert to txtField prompt before distributing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What's the main idea behind MVP?\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question as truthfully as possible, and if you're unsure of the answer, say \"Sorry, I don't know\".\n",
    "\n",
    "Q: How many sprints in a PI?\n",
    "A:\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Engineering to increase domain 'truthiness'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question as truthfully as possible using the provided text, and if the answer is not contained within the text below, say \"I don't know\"\n",
    "\n",
    "Context:\n",
    "A Minimum Viable Product (MVP) is a version of a working product that allows the team to learn from and \n",
    "interact with their customer with the least amount of effort. MVP attends to the core customer needs \n",
    "first and as soon as possible. It helps to validate needs, reduce risk, and help the programs course correct \n",
    "quickly, as needed. Rooted in concepts that emerged from the book “The Lean Startup” by Eric Ries, the core \n",
    "idea is to facilitate a better understanding of the customers needs and interests without committing or \n",
    "using a large number of resources or fully developing a product.\n",
    "\n",
    "Q: What's the main idea behind MVP?\n",
    "A:\"\"\"\n",
    "\n",
    "openai.Completion.create(\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    max_tokens=300,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    model=COMPLETIONS_MODEL\n",
    ")[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data from vstart.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('vstart.csv')\n",
    "df = df.set_index([\"vscontent\"])\n",
    "print(f\"{len(df)} rows in the data.\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str = EMBEDDING_MODEL) -> list[float]:\n",
    "    \"\"\"\n",
    "    Use the OpenAI Embeddings API to create an embedding for the given text.\n",
    "    \"\"\"\n",
    "    result = openai.Embedding.create(model=model, input=text)\n",
    "    return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame) -> dict[str, list[float]]:\n",
    "    \"\"\"\n",
    "    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n",
    "\n",
    "    Return a dictionary that maps between each embedding vector and the vscontent value of the row it corresponds to.\n",
    "    \"\"\"\n",
    "    return {r['vscontent']: get_embedding(r['vscontent']) for idx, r in df.iterrows()}\n",
    "\n",
    "def load_embeddings(fname: str) -> dict[str, list[float]]:\n",
    "    \"\"\"\n",
    "    Read the document embeddings from a CSV.\n",
    "\n",
    "    fname is the path to a CSV with exactly these named columns: \n",
    "        \"vscontent\", \"0\", \"1\", ... up to the length of the embedding vectors.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(fname, header=0)\n",
    "    max_dim = max([int(c) for c in df.columns if c != \"vscontent\"])\n",
    "    return {r[0]: [r[i] for i in range(1, max_dim + 1)] for r in df.itertuples(index=False)}\n",
    "\n",
    "df = pd.read_csv(\"vstart.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = compute_doc_embeddings(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets save our embeddings in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(embeddings: dict[str, list[float]], fname: str):\n",
    "    df = pd.DataFrame.from_dict(embeddings, orient='index')\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index': 'vscontent'}, inplace=True)\n",
    "    df.to_csv(fname, index=False)\n",
    "\n",
    "\n",
    "output_folder = \"output\"\n",
    "output_fname = f\"{output_folder}/embeddings.csv\"\n",
    "save_embeddings(document_embeddings, output_fname)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets view what an embedding looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_text = list(document_embeddings.keys())[0]\n",
    "first_embedding = list(document_embeddings.values())[0]\n",
    "truncated_text = first_text[:50] + \"...\"\n",
    "print(\"Example:\", truncated_text, first_embedding[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  We can use this for search results as well \n",
    "\n",
    "...by finding the most similar document embeddings to the question embedding. We're storing this locally but for a larger dataset we would consider a vector DB to index our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_similarity(x: list[float], y: list[float]) -> float:\n",
    "    \"\"\"\n",
    "    Returns the similarity between two vectors.\n",
    "    \n",
    "    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n",
    "    \"\"\"\n",
    "    return np.dot(np.array(x), np.array(y))\n",
    "\n",
    "def order_document_sections_by_query_similarity(query: str, contexts: dict[(str, str), np.array]) -> list[(float, (str, str))]:\n",
    "    \"\"\"\n",
    "    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n",
    "    to find the most relevant sections. \n",
    "    \n",
    "    Return the list of document sections, sorted by relevance in descending order.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    document_similarities = sorted([\n",
    "        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()\n",
    "    ], reverse=True)\n",
    "    \n",
    "    return document_similarities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets try a semantic search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_document_sections_by_query_similarity(\"how many sprints in a PI\", document_embeddings)[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
