{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Playground\n",
    "\n",
    "### Prompt chaining example for Pete, Pallavi, and Chris's dashboard report \"podcast\" idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain, openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Refactor to use utils.py\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Diligence Insights Consulting.\n"
     ]
    }
   ],
   "source": [
    "text = \"What would be a good company name for a consultancy that provides due dilligence consulting for startups and investment firms?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would be a good company name for a consultancy that provides due dilligence consulting for startups and investment firms?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "prompt = PromptTemplate (\n",
    "    input_variables=[\"service\"],\n",
    "    template=\"What would be a good company name for a consultancy that provides {service}?\",\n",
    ")\n",
    "\n",
    "# print(prompt.format(service=\"due dilligence consulting for startups and investment firms\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from  langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"service\"],\n",
    "    template=\"What would be a good company name for a consultancy that provides {service}?\",\n",
    ")\n",
    "\n",
    "#due dilligence consulting for startups and investment firms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nDueDiligence Consulting Group'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "chain.run (\"due dilligence consulting for startups and investment firms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"ce60e0346b62556704a180f5104bd1dcae3c1cdc0d8184c0f46072dd883e3709\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamically call chains based on user input with a LangChain AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n",
      "Action: Search\n",
      "Action Input: \"Olivia Wilde boyfriend\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mJason Sudeikis\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out Jason Sudeikis' age\n",
      "Action: Search\n",
      "Action Input: \"Jason Sudeikis age\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m47 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate 47 raised to the 0.23 power\n",
      "Action: Calculator\n",
      "Action Input: 47^0.23\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.4242784855673896\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Jason Sudeikis, Olivia Wilde's boyfriend, is 47 years old and his age raised to the 0.23 power is 2.4242784855673896.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Jason Sudeikis, Olivia Wilde's boyfriend, is 47 years old and his age raised to the 0.23 power is 2.4242784855673896.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# load lm\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "#load tools\n",
    "tools = load_tools([\"serpapi\",\"llm-math\"], llm=llm)\n",
    "\n",
    "#init agent w/ tools, LM, agent type\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "# run\n",
    "agent.run (\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short term memory example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. My name is AI. What's your name?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
      "Human: Nice to meet you as well. My name is Bonbon.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Nice to meet you, Bonbon! What can I do for you today?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Nice to meet you as well. My name is Bonbon.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default prompt template examples/sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "I want you to summarize this dashboard report. \n",
    "# Audience Overview\t\t\n",
    "# 20230220-20230226 20230213-20230219\t\t\n",
    "# ----------------------------------------\t\t\n",
    "\t\t\n",
    "Day Index\tDate Range\tUsers\n",
    "2/20/23\tFeb 20, 2023 - Feb 26, 2023\t5,631\n",
    "2/13/23\tFeb 13, 2023 - Feb 19, 2023\t6,812\n",
    "2/21/23\tFeb 20, 2023 - Feb 26, 2023\t6,703\n",
    "2/14/23\tFeb 13, 2023 - Feb 19, 2023\t6,739\n",
    "2/22/23\tFeb 20, 2023 - Feb 26, 2023\t6,262\n",
    "2/15/23\tFeb 13, 2023 - Feb 19, 2023\t6,616\n",
    "2/23/23\tFeb 20, 2023 - Feb 26, 2023\t4,055\n",
    "2/16/23\tFeb 13, 2023 - Feb 19, 2023\t6,504\n",
    "2/24/23\tFeb 20, 2023 - Feb 26, 2023\t3,377\n",
    "2/17/23\tFeb 13, 2023 - Feb 19, 2023\t6,189\n",
    "2/25/23\tFeb 20, 2023 - Feb 26, 2023\t354\n",
    "2/18/23\tFeb 13, 2023 - Feb 19, 2023\t3,382\n",
    "2/26/23\tFeb 20, 2023 - Feb 26, 2023\t386\n",
    "2/19/23\tFeb 13, 2023 - Feb 19, 2023\t3,282\n",
    "\tFeb 20, 2023 - Feb 26, 2023\t26,768\n",
    "\tFeb 13, 2023 - Feb 19, 2023\t39,524\n",
    "\n",
    "Here are some examples of a dashboard report summary:\n",
    "\n",
    "- Fewer users returned to your app in January, you had 44K users in Decemberm 6.03 thousand came back in January, which means 12.85 percent returned to your app. \n",
    "- During the previous week (February 13th - 19th), there were a total of 39,524 users. The highest number of users (6,812) accessed the site on February 13th, while the lowest number (3,282) accessed it on February 19th.\n",
    "- There was a decrease in the number of users accessing the site from the previous week to the current week.\n",
    "\n",
    "The summary should be short, descriptive and easy to understand.\n",
    "\n",
    "What is my website performance {summary}?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"summary\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Prompt Template example/sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "    \"answer\": \n",
    "\"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: How old was Muhammad Ali when he died?\n",
    "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "Follow up: How old was Alan Turing when he died?\n",
    "Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "So the final answer is: Muhammad Ali\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"When was the founder of craigslist born?\",\n",
    "    \"answer\": \n",
    "\"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the founder of craigslist?\n",
    "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
    "Follow up: When was Craig Newmark born?\n",
    "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
    "So the final answer is: December 6, 1952\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
    "    \"answer\":\n",
    "\"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the mother of George Washington?\n",
    "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
    "Follow up: Who was the father of Mary Ball Washington?\n",
    "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
    "So the final answer is: Joseph Ball\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
    "    \"answer\":\n",
    "\"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who is the director of Jaws?\n",
    "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
    "Follow up: Where is Steven Spielberg from?\n",
    "Intermediate Answer: The United States.\n",
    "Follow up: Who is the director of Casino Royale?\n",
    "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
    "Follow up: Where is Martin Campbell from?\n",
    "Intermediate Answer: New Zealand.\n",
    "So the final answer is: No\n",
    "\"\"\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(examples, OpenAIEmbeddings, Chroma, k=1)\n",
    "\n",
    "\n",
    "# Select the most similar example to the input.\n",
    "question = \"Who was the father of Mary Ball Washington?\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "print(f\"Examples most similar to the input: {question}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
